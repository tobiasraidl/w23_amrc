---
title: "Exercise 2"
author: "Tobias Raidl, 11717659"
date: "2023-10-02"
output: pdf_document
header-includes:
  \usepackage{fvextra}
  \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, tidy=TRUE)
```

### Preprocessing
Set up train and test set
Omit NAs
Log transform
Remove variables "Accept" and "Enroll"
```{r}
library(dplyr)
# df name is College
data(College,package="ISLR")
College = na.omit(College)
df = select(College, -Accept, -Enroll)
df$Private = ifelse(df$Private == "Yes", 1, 0)
apps = log(df$Apps)
df$Apps = apps
#df = data.frame(scale(select(df,-Private)))
#df = cbind("Private"=private,df)
df$Apps = log(df$Apps)
set.seed(11717659)
sample = sample(c(TRUE, FALSE), size=nrow(df), replace=TRUE, prob=c(2/3,1/3))
train = df[sample, ]
y_train = train$Apps
test = df[!sample, ]
y_test = test$Apps
test = select(test, -Apps)
```
### 1.a
The variables contributing to explaining the variable "Apps" the most are no necessarily the ones with the highest absolute coefficient. Say we use an alpha of 0.05, all the variables with at least one asterisk to their name are perceived as statistically significant. The assumptions are not valid, because for high fitted values the variability is far lower than for small ones. Also the qq plot shows that the distribution is not normal and therefore this requirement is not fulfilled. The leverage plot shows possible outliars (outliars indicated by cooks distance > 1), we got none. Preprocessing the observed variables in addition to the response variable could make these plots more meaningful.
```{r}
my_model = lm(Apps ~., data=train)
summary(my_model)
plot(my_model)

```

### 1.b
I encoded the boolean variable "Private" to Yes=1 and No=0. Because I did only log transform the Apps variable, the coefficients do not necessarily tell how much each variable describes the "Apps" variable.
```{r}
X = model.matrix(Apps ~., data=train)
b = solve(t(X) %*% X) %*% t(X) %*% y_train
b
```

# 1.c
I chose to visualize the predicted values for train and test set by using a boxplot for the distribution of the absolute differences between ground truth and each dataset. As expected, the median of the train set differences goes down. Interestingly the 3rd median and the outliars are going up. As outliars do not belong to the same distribution though, they can be neglected.
```{r}
y_hat_train = unlist(predict(my_model, select(train, -Apps)))
diffs = abs(y_train-y_hat_train)
par(mfrow = c(1, 2))
boxplot(main="pred x gt differences (training set)", diffs, ylim=c(0,0.3))

y_hat_test = unlist(predict(my_model, test))
diffs = abs(y_test-y_hat_test)
boxplot(main="pred x gt differences (test set)", diffs, ylim=c(0,0.3))
```

# 1.d
The error for the evaluation on the test set is higher due to the model being fitted on the exact train set data. This means it has a bias towards the train set and therefore a lower error.
```{r}


get_rmse = function(model, n, X, y) {
  y_hat = unlist(predict(model, X))
  rmse = sqrt((1/n) * sum((y-y_hat)^2))
  return(rmse)
}

train_rmse = get_rmse(my_model, nrow(train), select(train, -Apps), y_train)
test_rmse = get_rmse(my_model, nrow(test), test, y_test)

cat(paste("Train RMSE: ", train_rmse, "\nTest RMSE: ", test_rmse))

```

# 2.a
Say alpha=0.05
Not all variables that were previously significant are the same now. Expend is is not significant. It is not necessarily expected, because the test statistic is a new one too, meaning different p-values.
```{r}
small_train = select(train, c(Apps, Private,Top10perc, F.Undergrad, Outstate, Room.Board, perc.alumni, Expend, Grad.Rate))
small_test = select(test, c(Private,Top10perc, F.Undergrad, Outstate, Room.Board, perc.alumni, Expend, Grad.Rate))
small_model = lm(Apps ~., data=small_train)
summary(small_model)
plot(small_model)
```

# 2.b
```{r}
y_hat_small_train = unlist(predict(small_model, select(small_train, -Apps)))
diffs = abs(y_train-y_hat_small_train)
#par(mfrow = c(1, 2))
boxplot(main="pred x gt differences (training set)", diffs, ylim=c(0,0.4))

y_hat_small_test = unlist(predict(small_model, small_test))
diffs = abs(y_test-y_hat_small_test)
boxplot(main="pred x gt differences (test set)", diffs, ylim=c(0,0.4))
```

# 2.c
Again we evaluate for both the training and the test set, expecting the model to perform better on the train set. The errors are pretty similar to the ones of the model with all variables. The chosen variables seem to cover a big amount of the explanatory power for Apps.
```{r}
get_rmse = function(model, n, X, y) {
  y_hat = unlist(predict(model, X))
  rmse = sqrt((1/n) * sum((y-y_hat)^2))
  return(rmse)
}

train_rmse = get_rmse(small_model, nrow(small_train), select(small_train, -Apps), y_train)
test_rmse = get_rmse(small_model, nrow(small_test), small_test, y_test)

cat(paste("Train RMSE: ", train_rmse, "\nTest RMSE: ", test_rmse))

```

# 2.d
ANOVA for checking if the two samples. The extremely low p-value means that the difference of the models is. Google says an F-value of over 2.5 would suggest to reject the null hypothesis. Ours i ~6.9.

I think the abbreviations mean the following: RSS: sum of squared errors (between each model prediction and observed value), Sum of Sq: sum of squared differences between models
```{r}
anova(my_model, small_model)
```

# 3.
Both the RMSE aswell as the y vs. y hat plots are similar even though they backward and forward step algorithm came not to the same conclusion of the "optimal" features.
```{r}
model_empty = lm(Apps ~ 1, data=train)
model_backward = step(my_model, direction="backward", trace=0)
model_forward = step(model_empty, scope=formula(my_model), direction="forward", trace=0)

y_hat_backward = predict(model_backward, test)
y_hat_forward = predict(model_forward, test)

cat(paste("backward model rmse: ", get_rmse(model_backward, nrow(test), test, y_test)))
cat(paste("\nforward model rmse: ", get_rmse(model_forward, nrow(test), test, y_test)))

par(mfrow=c(1,2))
plot(x=y_test, y=y_hat_backward)
plot(x=y_test, y=y_hat_forward)

```