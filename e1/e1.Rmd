---
title: "Exercise 2"
author: "Tobias Raidl, 11717659"
date: "2023-10-02"
output: pdf_document
header-includes:
  \usepackage{fvextra}
  \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, tidy=TRUE)
```

##
Set up train and test set
Omit NAs
Log transform and standardize features "Accept" and "Enroll"
```{r}
library(dplyr)
# df name is College
data(College,package="ISLR")
College = na.omit(College)
df = select(College, -Accept, -Enroll)
df$Private = ifelse(df$Private == "Yes", 1, 0)
private = df$Private
df = data.frame(scale(select(df,-Private)))
df = cbind("Private"=private,df)

sample = sample(c(TRUE, FALSE), size=nrow(df), replace=TRUE, prob=c(2/3,1/3))
train = df[sample, ]
y_train = train$Apps
train
test = df[!sample, ]
y_test = test$Apps
test = select(test, -Apps)
test
```
### a
The variables contributing to explaining the variable "Apps" the most are the ones with the highest absolute coefficient. In our case the top 3 are "F. Undergrad", "Expend" and "Private". Yes the assumptions of the diagnostics plots with `plot(res)` are valid.
```{r}
my_model = lm(Apps ~., data=train)
summary(res)
plot(res)

```

### b
I encoded the boolean variable "Private" to Yes=1 and No=0. Because I standardized all the variables in advance (besides private), the coefficients tell how much each variable describes the "Applications" variable. A high absolute coefficient in comparison to the other coefficients means that its corresponding variable has high explanatory power. "F. Undergrad" for example has a high coefficient, which implies that colleges with a high number in "F. Undergrad" would be estimated by this linear regression model to have a higher "Applications" number on average.
```{r}
X = model.matrix(Apps ~., data=train)
b = solve(t(X) %*% X) %*% t(X) %*% y_train
b
```

# c
I chose to visualize the predicted values for train and test set by using a boxplot for the distribution of the absolute differences between ground truth and each dataset. As expected, the median of the train set differences goes down. Interestingly the 3rd median and the outliars are going up. As outliars do not belong to the same distribution though, they can be neglected.
```{r}
y_hat_train = unlist(predict(my_model, select(train, -Apps)))
diffs = abs(y_train-y_hat_train)
boxplot(main="prediction x ground truth differences evaluated on training set", diffs, ylim=c(0,2))

y_hat_test = unlist(predict(my_model, test))
diffs = abs(y_test-y_hat_test)
boxplot(main="prediction x ground truth differences evaluated on test set", diffs, ylim=c(0,2))
```

# d
The error for the evaluation on the test set is higher due to the model being fitted on the exact train set data. This means it has a bias towards the train set and therefore a lower error.
```{r}


get_rmse = function(model, n, X, y) {
  y_hat = unlist(predict(my_model, train))
  rmse_train = sqrt((1/n_train) * sum((y-y_hat)^2))
  return(rmse_train)
}

train_rmse = get_rmse(my_model, nrow(train), select(train, -Apps), y_train)
test_rmse = get_rmse(my_model, nrow(test), test, y_test)

cat(paste("Train RMSE: ", train_rmse, "\nTest RMSE: ", test_rmse))

```

