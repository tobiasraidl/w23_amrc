---
title: "Exercise 2"
author: "Tobias Raidl, 11717659"
date: "2023-11-02"
output: pdf_document
header-includes:
  \usepackage{fvextra}
  \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, tidy=TRUE)
```

Setup data
```{r}
load("building.RData")
set.seed(11717659)
df
sample = sample(c(TRUE, FALSE), nrow(df), replace=TRUE, prob=c(2/3,1/3))
train = df[sample, ]
test = df[!sample, ]
```

# 1
## (a)
```{r}
library(dplyr)
library(ggplot2)
model = lm(y~., train)
train.y_pred = predict(model, select(train, -y))
res.train = data.frame(y=train$y, y_pred=train.y_pred)
ggplot(res.train, aes(x=y, y=y_pred)) +
  geom_point()

get_rmse = function(y, y_pred) {
  residuals = (y-y_pred)^2
  return(sqrt(sum(residuals)/length(residuals)))
}

cat(paste("RMSE for train set:", get_rmse(res.train$y, res.train$y_pred)))
```

## (b)
```{r}
test.y_pred = predict(model, select(test, -y))
res.test = data.frame(y=test$y, y_pred=test.y_pred)
ggplot(res.test, aes(x=y, y=y_pred)) +
  geom_point()

get_rmse = function(y, y_pred) {
  residuals = (y-y_pred)^2
  return(sqrt(sum(residuals)/length(residuals)))
}

cat(paste("RMSE for test set:", get_rmse(res.test$y, res.test$y_pred)))
```

## (c)
The RMSE is higher for the test set than the train set. This is due to the model being fitted on the train set. A high error difference between these to sets indicates an overfitted model.
```{r}
ggplot() +
  geom_point(data=res.test, mapping=aes(x=y, y=y_pred), color="brown", pch="o") +
  geom_point(data=res.train, mapping=aes(x=y, y=y_pred), color="blue", pch="o") +
  ggtitle("y vs predicted y for train(brown) and test(blue) set")
```

## (d)
I chose a 50/50 split and the RMSE for the train set is even lower, but the rmse for the test split higher than with the previous split. More training data being available translates into a more overfitted model. But also more test data being available means that the evaluation is probably more accurate. than before.
```{r}
set.seed(11717659)
sample2 = sample(c(TRUE, FALSE), nrow(df), replace=TRUE, prob=c(0.5, 0.5))
train2 = df[sample2, ]
test2 = df[!sample2, ]

model = lm(y~., train2)
train2.y_pred = predict(model, select(train2, -y))
res.train2 = data.frame(y=train2$y, y_pred=train2.y_pred)
ggplot(res.train2, aes(x=y, y=y_pred)) +
  geom_point()

get_rmse = function(y, y_pred) {
  residuals = (y-y_pred)^2
  return(sqrt(sum(residuals)/length(residuals)))
}

test2.y_pred = predict(model, select(test2, -y))
res.test2 = data.frame(y=test2$y, y_pred=test2.y_pred)
ggplot(res.test2, aes(x=y, y=y_pred)) +
  geom_point()

get_rmse = function(y, y_pred) {
  residuals = (y-y_pred)^2
  return(sqrt(sum(residuals)/length(residuals)))
}

cat(paste("RMSE for train set:", get_rmse(res.train2$y, res.train2$y_pred)))
cat("\n")
cat(paste("RMSE for test set:", get_rmse(res.test2$y, res.test2$y_pred)))
```

# 2
Nothin to do here

# 3
## (a)
```{r}
library(pls)
set.seed(11717659)
# train_idxs = which(sample)
pcr_fit = pcr(y~., data=train, scale=TRUE, validation="CV", segments=10, segment.type="random")
summary(pcr_fit)
```

## (b)
34 components seem to be optimal
```{r}
validationplot(pcr_fit)
predplot(pcr_fit)
```

## (c)
```{r}
predplot(pcr_fit, ncomp=34, line=TRUE)
```

## (d)
```{r}
predplot(pcr_fit, newdata=test, ncomp=34, line=TRUE)
```

# 4
## (a)
```{r}
plsr_fit = plsr(y~., data=train, scale=TRUE, validation="CV", segments=10, segment.type="random")
summary(plsr_fit)
```

## (b)
14 components seem to be optimal
```{r}
validationplot(plsr_fit)
```

## (c)
```{r}
predplot(plsr_fit, ncomp=14, line=TRUE)
```

## (d)
```{r}
predplot(plsr_fit, newdata=test, ncomp=34, line=TRUE)
```

## (e)
```{r}
coefplot(pcr_fit)
coefplot(plsr_fit)
```
